import asyncio
import json
import os
import sys
from typing import Optional, List, Dict, Any
from datetime import datetime
import aiohttp
import google.generativeai as genai
import requests

# æ·»åŠ google_crawlç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'google_crawl'))

from backend.models.token import TokenData, AnalysisResult, NarrativeAnalysis, RiskLevel, MarketAnalysis, WebSearchResult,SimpleAnalysisResult
from backend.services.message_queue import MessageQueue
from backend.utils.logger import setup_logger
from backend.utils.env_loader import get_env_var
# uvicorn backend.main:app --host 0.0.0.0 --port 8000 --reloa
# å¯¼å…¥é«˜æ•ˆGoogleæœç´¢å¼•æ“
try:
    from backend.google_engine import search_crypto_info, SearchResult
    GOOGLE_ENGINE_AVAILABLE = True
    logger = setup_logger(__name__)
    logger.info("âœ… é«˜æ•ˆGoogleæœç´¢å¼•æ“å·²åŠ è½½")
except ImportError as e:
    GOOGLE_ENGINE_AVAILABLE = False
    logger = setup_logger(__name__)
    logger.warning(f"âš ï¸ é«˜æ•ˆGoogleæœç´¢å¼•æ“ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ç®€åŒ–æœç´¢: {e}")

logger = setup_logger(__name__)

class AIAnalyzer:
    """AIåˆ†ææœåŠ¡ï¼Œä½¿ç”¨Geminiè¿›è¡Œä»£å¸åˆ†æ"""

    def __init__(self, message_queue: MessageQueue, max_concurrent_ai_requests: int = 1):
        self.message_queue = message_queue
        self.is_running = False
        self.gemini_model = None

        # AIå¹¶å‘æ§åˆ¶
        self.max_concurrent_ai_requests = max_concurrent_ai_requests
        self.ai_semaphore = asyncio.Semaphore(max_concurrent_ai_requests)
        self.active_ai_requests = 0
        self.ai_request_lock = asyncio.Lock()

        # ç»Ÿè®¡ä¿¡æ¯
        self.total_requests = 0
        self.completed_requests = 0
        self.failed_requests = 0

        logger.info(f"AIåˆ†æå™¨åˆå§‹åŒ–ï¼Œæœ€å¤§å¹¶å‘AIè¯·æ±‚æ•°: {max_concurrent_ai_requests}")
        # ä¸å†éœ€è¦Google APIå¯†é’¥ï¼Œä½¿ç”¨é«˜æ•ˆçˆ¬è™«
        
    async def initialize(self):
        """åˆå§‹åŒ–AIæœåŠ¡"""
        # é…ç½®Gemini API
        try:
            gemini_api_key = get_env_var("GEMINI_API_KEY", required=True)
            logger.info(f"use gemini key {gemini_api_key[:8]}...")

            genai.configure(api_key=gemini_api_key)
            # ä½¿ç”¨æœ€æ–°çš„Geminiæ¨¡å‹åç§°
            self.gemini_model = genai.GenerativeModel('gemini-2.0-flash')

            logger.info("Gemini AIæœåŠ¡åˆå§‹åŒ–å®Œæˆ")

            tweet_endpoint = get_env_var("TWITTER_API_ENDPOINT",required=True)
            logger.info("æˆåŠŸæ‰¾ tweet æœç´¢è·¯ç”±")
            self.tweet_endpoint = tweet_endpoint
        except ValueError as e:
            logger.error(f"âŒ Gemini APIåˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    async def _controlled_ai_request(self, request_func, *args, **kwargs):
        """å—å¹¶å‘æ§åˆ¶çš„AIè¯·æ±‚åŒ…è£…å™¨"""
        async with self.ai_semaphore:  # è·å–ä¿¡å·é‡ï¼Œé™åˆ¶å¹¶å‘æ•°
            async with self.ai_request_lock:
                self.active_ai_requests += 1
                self.total_requests += 1
                current_active = self.active_ai_requests

            logger.info(f"ğŸ¤– å¼€å§‹AIè¯·æ±‚ (æ´»è·ƒ: {current_active}/{self.max_concurrent_ai_requests}, æ€»è®¡: {self.total_requests})")

            try:
                # æ‰§è¡Œå®é™…çš„AIè¯·æ±‚
                result = await request_func(*args, **kwargs)

                async with self.ai_request_lock:
                    self.completed_requests += 1

                logger.info(f"âœ… AIè¯·æ±‚å®Œæˆ (å®Œæˆ: {self.completed_requests}, å¤±è´¥: {self.failed_requests})")
                return result

            except Exception as e:
                async with self.ai_request_lock:
                    self.failed_requests += 1

                logger.error(f"âŒ AIè¯·æ±‚å¤±è´¥: {e} (å®Œæˆ: {self.completed_requests}, å¤±è´¥: {self.failed_requests})")
                raise

            finally:
                async with self.ai_request_lock:
                    self.active_ai_requests -= 1
        
    async def start_consumer(self):
        """å¯åŠ¨æ¶ˆè´¹è€…ï¼Œå¤„ç†åˆ†æä»»åŠ¡"""
        self.is_running = True
        logger.info("AIåˆ†ææ¶ˆè´¹è€…å·²å¯åŠ¨")
        
        while self.is_running:
            try:
                task = await self.message_queue.get_analysis_task()
                if task:
                    # å¼‚æ­¥å¤„ç†åˆ†æä»»åŠ¡
                    asyncio.create_task(self._process_analysis_task(task))
                else:
                    # æ²¡æœ‰ä»»åŠ¡æ—¶çŸ­æš‚ä¼‘çœ 
                    await asyncio.sleep(0.1)
            except Exception as e:
                logger.error(f"æ¶ˆè´¹è€…å¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {e}")
                await asyncio.sleep(1)
                
    async def stop(self):
        """åœæ­¢AIåˆ†ææœåŠ¡"""
        self.is_running = False
        logger.info("AIåˆ†ææœåŠ¡å·²åœæ­¢")
        
    async def _process_analysis_task(self, task: Dict[str, Any]):
        """å¤„ç†å•ä¸ªåˆ†æä»»åŠ¡"""
        try:
            token_data_dict = task["token_data"]
        except:
            logger.error("_process_analysis_task error {e}")
            logger.error(f"task = {task}")
        token_data = TokenData(**token_data_dict)
        
        logger.info(f"å¼€å§‹åˆ†æä»£å¸: {token_data.symbol}")
        
        try:
            # æ›´æ–°çŠ¶æ€ä¸ºåˆ†æä¸­
            await self.message_queue.update_analysis_progress(
                token_data.mint, 10.0, "ANALYZING"
            )
            
            # 1. ç½‘ç»œæœç´¢
            search_results = await self._search_token_info(token_data)
            await self.message_queue.update_analysis_progress(token_data.mint, 30.0)

            # 2. æ¨æ–‡åˆ†æ
            tweet_analysis = await self._analyze_tweets(token_data)
            await self.message_queue.update_analysis_progress(token_data.mint, 50.0)

            # 3. ç®€å•åˆ†æ
            simple_analysis = await self._ai_analyze_simple(token_data,search_results,tweet_analysis)
            
            # 3. å™äº‹åˆ†æ
            # narrative_analysis = await self._analyze_narrative(token_data, search_results)
            # await self.message_queue.update_analysis_progress(token_data.mint, 70.0)
            
            
            # 3. é£é™©è¯„ä¼°
            # risk_assessment = await self._assess_risk(token_data, search_results)
            # await self.message_queue.update_analysis_progress(token_data.mint, 70.0)
            
            # 4. å¸‚åœºåˆ†æ
            # market_analysis = await self._analyze_market(token_data)
            # await self.message_queue.update_analysis_progress(token_data.mint, 85.0)
            
            # 5. ç”ŸæˆAIæ€»ç»“
            # ai_summary = await self._generate_summary(
            #     token_data, narrative_analysis, risk_assessment, market_analysis, search_results
            # )
            # await self.message_queue.update_analysis_progress(token_data.mint, 95.0)
            
            # 6. æŠ•èµ„å»ºè®®
            # investment_recommendation = await self._generate_investment_recommendation(
            #     token_data, narrative_analysis, risk_assessment, market_analysis
            # )
            
            # åˆ›å»ºå®Œæ•´çš„åˆ†æç»“æœ
            # analysis_result = AnalysisResult(
            #     token_mint=token_data.mint,
            #     token_symbol=token_data.symbol,
            #     token_name=token_data.name,
            #     status="COMPLETED",
            #     progress=100.0,
            #     narrative_analysis=narrative_analysis,
            #     risk_assessment=risk_assessment,
            #     market_analysis=market_analysis,
            #     web_search_results=search_results,
            #     ai_summary=ai_summary,
            #     investment_recommendation=investment_recommendation,
            #     analysis_completed_at=datetime.now()
            # )
            analysis_result = AnalysisResult(
                token_mint=token_data.mint,
                token_symbol=token_data.symbol,
                token_name=token_data.name,
                status="COMPLETED",
                progress=100.0,
                narrative_analysis=simple_analysis.narrative_analysis,
                risk_assessment=simple_analysis.risk_assessment,
                market_analysis=simple_analysis.market_analysis,
                web_search_results=search_results,
                tweet_result=[{"content":tweet_analyse['content'] if len(tweet_analyse['content'])<100 else tweet_analyse['content'][:100]+"...","link":tweet_analyse["t_url"]} for tweet_analyse in tweet_analysis],
                ai_summary=simple_analysis.ai_summary,
                investment_recommendation=simple_analysis.investment_recommendation,
                analysis_completed_at=datetime.now()
            )
            
            # å®Œæˆåˆ†æ
            await self.message_queue.complete_analysis(analysis_result)
            
        except Exception as e:
            logger.error(f"åˆ†æä»£å¸ {token_data.symbol} æ—¶å‡ºé”™: {e}")
            await self.message_queue.fail_analysis(token_data.mint, str(e))
            
    async def _analyze_tweets(self, token_data:TokenData) -> List[Dict[str,Any]]:
        def search_tweet(reqBody) -> List[Dict[str,Any]]:
            data = requests.post(self.tweet_endpoint,json=reqBody)
            if data.status_code==200:
                return data.json()['data']
            else:
                logger.error(f"æ”¶åˆ°é”™è¯¯çš„æ¨ç‰¹æœç´¢ç»“æœ {data.json()}")
        try:
            reqBody = {
                "keyword":f"{token_data.mint}"
            }
            logger.info(f"use tweet search for ${token_data.symbol}({token_data.mint})")
            loop = asyncio.get_event_loop()
            # å°†åŒæ­¥æ“ä½œæ”¾å…¥çº¿ç¨‹æ± ï¼Œé˜²æ­¢é˜»å¡å…¶ä»–åç¨‹
            tweet_res = await loop.run_in_executor(
                None,
                search_tweet,
                reqBody
            )
            logger.info(f"æ¨ç‰¹æœç´¢æˆåŠŸï¼Œå¾—åˆ° {len(tweet_res)} ä¸ªç»“æœ")
            return tweet_res

        except Exception as e:
            logger.error(f"æ¨ç‰¹æœç´¢å¼•æ“å¤±è´¥")
            return []

        
    async def _search_token_info(self, token_data: TokenData) -> List[WebSearchResult]:
        """æœç´¢ä»£å¸ç›¸å…³ä¿¡æ¯ - ä½¿ç”¨é«˜æ•ˆGoogleæœç´¢å¼•æ“"""
        all_results = []

        # æ–¹æ¡ˆ1: ä½¿ç”¨é«˜æ•ˆGoogleæœç´¢å¼•æ“
        if GOOGLE_ENGINE_AVAILABLE:
            try:
                logger.info(f"ğŸ” ä½¿ç”¨é«˜æ•ˆGoogleæœç´¢å¼•æ“æœç´¢: {token_data.symbol}")

                # ä½¿ç”¨å¼‚æ­¥æ–¹å¼è¿è¡ŒåŒæ­¥çš„æœç´¢å‡½æ•°
                loop = asyncio.get_event_loop()
                search_results = await loop.run_in_executor(
                    None,
                    search_crypto_info,
                    token_data.symbol,
                    token_data.mint,
                    6  # è·å–12ä¸ªç»“æœ
                )

                logger.info(f"âœ… Googleæœç´¢è·å¾— {len(search_results)} ä¸ªç»“æœ")

                # è½¬æ¢ä¸ºWebSearchResultæ ¼å¼
                for i, result in enumerate(search_results):
                    try:
                        web_result = WebSearchResult(
                            title=result.title or "æ— æ ‡é¢˜",
                            url=result.url or "",
                            snippet=result.description or "æ— æè¿°",
                            relevance_score=max(95.0 - i * 3, 60.0)  # æ ¹æ®æ’åè®¡ç®—ç›¸å…³æ€§
                        )
                        all_results.append(web_result)
                    except Exception as e:
                        logger.warning(f"è½¬æ¢æœç´¢ç»“æœå¤±è´¥: {e}")

            except Exception as e:
                logger.error(f"Googleæœç´¢å¼•æ“å¤±è´¥: {e}")

        # æ–¹æ¡ˆ2: å¦‚æœGoogleæœç´¢å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–æœç´¢
        if not all_results:
            logger.info("ğŸ” Googleæœç´¢å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–æœç´¢æ–¹æ¡ˆ")
            all_results = await self._simple_search(token_data)

        logger.info(f"ğŸ” æ€»å…±è·å¾— {len(all_results)} ä¸ªæœç´¢ç»“æœ")
        return all_results[:15]  # è¿”å›å‰15ä¸ªæœ€ç›¸å…³çš„ç»“æœ

    async def _simple_search(self, token_data: TokenData) -> List[WebSearchResult]:
        """ç®€åŒ–æœç´¢æ–¹æ¡ˆ - ç”ŸæˆåŸºäºä»£å¸ä¿¡æ¯çš„æ¨¡æ‹Ÿæœç´¢ç»“æœ"""
        results = []

        # åŸºäºä»£å¸ä¿¡æ¯ç”Ÿæˆç›¸å…³çš„æœç´¢ç»“æœ
        base_results = [
            {
                "title": f"{token_data.name} ({token_data.symbol}) - Pump.funä»£å¸ä¿¡æ¯",
                "url": f"https://pump.fun/coin/{token_data.mint}",
                "snippet": f"{token_data.name}æ˜¯ä¸€ä¸ªåœ¨Pump.funå¹³å°ä¸Šå‘å¸ƒçš„ä»£å¸ï¼Œç¬¦å·ä¸º{token_data.symbol}ã€‚æ€»ä¾›åº”é‡ï¼š{token_data.token_total_supply}ã€‚",
                "relevance": 95.0
            },
            {
                "title": f"{token_data.symbol} ä»£å¸åˆ†æ - åŠ å¯†è´§å¸å¸‚åœº",
                "url": f"https://coinmarketcap.com/currencies/{token_data.symbol.lower()}",
                "snippet": f"æŸ¥çœ‹{token_data.name} ({token_data.symbol})çš„æœ€æ–°ä»·æ ¼ã€å¸‚å€¼å’Œäº¤æ˜“æ•°æ®ã€‚è¿™æ˜¯ä¸€ä¸ªæ–°å…´çš„åŠ å¯†è´§å¸é¡¹ç›®ã€‚",
                "relevance": 85.0
            },
            {
                "title": f"{token_data.name} ç¤¾åŒºè®¨è®º - Reddit",
                "url": f"https://reddit.com/r/cryptocurrency/search?q={token_data.symbol}",
                "snippet": f"Redditç¤¾åŒºå¯¹{token_data.name} ({token_data.symbol})çš„è®¨è®ºå’Œåˆ†æã€‚äº†è§£ç¤¾åŒºå¯¹è¯¥é¡¹ç›®çš„çœ‹æ³•ã€‚",
                "relevance": 75.0
            },
            {
                "title": f"{token_data.symbol} æŠ€æœ¯åˆ†æ - CoinGecko",
                "url": f"https://coingecko.com/en/coins/{token_data.symbol.lower()}",
                "snippet": f"{token_data.name}çš„æŠ€æœ¯æŒ‡æ ‡ã€ä»·æ ¼å†å²å’Œå¸‚åœºæ•°æ®åˆ†æã€‚è™šæ‹Ÿå‚¨å¤‡ï¼š{token_data.virtual_token_reserves}ã€‚",
                "relevance": 80.0
            },
            {
                "title": f"å¦‚ä½•è´­ä¹° {token_data.name} ({token_data.symbol})",
                "url": f"https://dexscreener.com/solana/{token_data.mint}",
                "snippet": f"åœ¨DEXä¸Šäº¤æ˜“{token_data.name}çš„æŒ‡å—ã€‚å½“å‰è™šæ‹ŸSOLå‚¨å¤‡ï¼š{token_data.virtual_sol_reserves}ã€‚",
                "relevance": 70.0
            }
        ]

        # è½¬æ¢ä¸ºWebSearchResultæ ¼å¼
        for result_data in base_results:
            try:
                result = WebSearchResult(
                    title=result_data["title"],
                    url=result_data["url"],
                    snippet=result_data["snippet"],
                    relevance_score=result_data["relevance"]
                )
                results.append(result)
            except Exception as e:
                logger.warning(f"åˆ›å»ºæœç´¢ç»“æœå¤±è´¥: {e}")

        logger.info(f"âœ… ç”Ÿæˆäº† {len(results)} ä¸ªæ¨¡æ‹Ÿæœç´¢ç»“æœ")
        return results

    async def _ai_analyze_simple(self,token_data,search_results:List[WebSearchResult],tweet_results:List[Dict[str,Any]])->SimpleAnalysisResult:
        """åˆ†æä»£å¸å™äº‹"""
        async def _do_ai_analyze_simple():
            logger.info(f"enter _do_ai_analyze_simple result = {search_results}")
            google_context = "\n".join([
                f"æ ‡é¢˜: {result.title}\næ‘˜è¦: {result.snippet}\né“¾æ¥: {result.url}"
                for result in search_results[:5]
            ])
            
            tweet_context = "\n".join([
                f"æ¨æ–‡å†…å®¹: {tweet_result['content']}\næ¨æ–‡é“¾æ¥: {tweet_result['t_url']}\nlike:{tweet_result['favorite_count']} retweet:{tweet_result['retweet_count']} reply:{tweet_result['reply_count']}"
                for tweet_result in tweet_results
            ])
            logger.info(f"tweet_context = {tweet_context}")
            prompt = f"""
            åˆ†æä»¥ä¸‹åŠ å¯†è´§å¸ä»£å¸çš„å™äº‹èƒŒæ™¯ï¼š
            
            - ä»£å¸åç§°: {token_data.name}
            - ä»£å¸ç¬¦å·: {token_data.symbol}
            - æ€»ä¾›åº”é‡: {token_data.token_total_supply:,}
            - è™šæ‹Ÿä»£å¸å‚¨å¤‡: {token_data.virtual_token_reserves:,}
            - è™šæ‹ŸSOLå‚¨å¤‡: {token_data.virtual_sol_reserves:,}
            - å®é™…ä»£å¸å‚¨å¤‡: {token_data.real_token_reserves:,}
            
            ç½‘ç»œæœç´¢ç»“æœ:
            {google_context}
            
            æ¨æ–‡ç»“æœ:
            {tweet_context}
            
            è¯·åˆ†æï¼š
                1. é¡¹ç›®çš„æ ¸å¿ƒå™äº‹å’Œæ¦‚å¿µ
                2. é£é™©è¯„åˆ†: 0-100 (100ä¸ºæœ€é«˜é£é™©)
                3. ä»·æ ¼é¢„æµ‹
                4. ç®€æ´ä½†å…¨é¢çš„æ€»ç»“
                5. æŠ•èµ„å»ºè®®
            è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
            {{
                "narrative_analysis": "å™äº‹åˆ†æ",
                "risk_assessment": "é£é™©è¯„ä¼°",
                "market_analysis": "å¸‚åœºåˆ†æ",
                "ai_summary": "AIæ€»ç»“",
                "investment_recommendation": "æŠ•èµ„å»ºè®®"
            }}
            """
            logger.info(f"gemini simple analyze prompt {prompt}")
            try:
                response = await asyncio.to_thread(
                    self.gemini_model.generate_content,
                    prompt
                )
                logger.info(f"gemini simple analyze response {response.text}")
                return response.text.strip()
            except Exception as e:
                logger.error(f"Gemini APIè°ƒç”¨å¤±è´¥: {e}")
                raise
        try:
            result_text = await self._controlled_ai_request(_do_ai_analyze_simple)
            logger.info(f"simple analyze results {result_text}")
            if result_text.startswith("```json"):
                result_text = result_text[7:-3]
            elif result_text.startswith("```"):
                result_text = result_text[3:-3]
            result_data = json.loads(result_text)
            return SimpleAnalysisResult(**result_data)
        except Exception as e:
            logger.error(f"_ai_analyze_simple error {e}")
            return SimpleAnalysisResult(
                narrative_analysis="å™äº‹åˆ†æå¤±è´¥",
                risk_assessment="é£é™©è¯„ä¼°å¤±è´¥",
                market_analysis="å¸‚åœºåˆ†æå¤±è´¥",
                ai_summary="AIæ€»ç»“å¤±è´¥",
                investment_recommendation="æŠ•èµ„å»ºè®®å¤±è´¥"
            )


        



    async def _analyze_narrative(self, token_data: TokenData, search_results: List[WebSearchResult]) -> NarrativeAnalysis:
        """åˆ†æä»£å¸å™äº‹"""
        search_context = "\n".join([
            f"æ ‡é¢˜: {result.title}\næ‘˜è¦: {result.snippet}\né“¾æ¥: {WebSearchResult.url}"
            for result in search_results[:5]
        ])
        
        prompt = f"""
        åˆ†æä»¥ä¸‹åŠ å¯†è´§å¸ä»£å¸çš„å™äº‹èƒŒæ™¯ï¼š
        
        ä»£å¸åç§°: {token_data.name}
        ä»£å¸ç¬¦å·: {token_data.symbol}
        
        ç½‘ç»œæœç´¢ç»“æœ:
        {search_context}
        
        è¯·åˆ†æï¼š
        1. ä»£å¸å±äºå“ªä¸ªç±»åˆ« (MEME, DEFI, AI, GAMING, NFT, ç­‰)
        2. é¡¹ç›®çš„æ ¸å¿ƒå™äº‹å’Œæ¦‚å¿µ
        3. å™äº‹çš„ç›¸å…³æ€§è¯„åˆ† (0-100)
        4. å½“å‰è¶‹åŠ¿åˆ†æ
        
        è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
        {{
            "category": "ç±»åˆ«",
            "description": "å™äº‹æè¿°",
            "relevance_score": è¯„åˆ†,
            "trend_analysis": "è¶‹åŠ¿åˆ†æ"
        }}
        """
        
        # å®šä¹‰å®é™…çš„AIè¯·æ±‚å‡½æ•°
        async def _do_narrative_analysis():
            try:
                # å°†åŒæ­¥å‡½æ•°è½¬åŒ–æˆå¼‚æ­¥å‡½æ•°ï¼Œé˜²æ­¢é˜»å¡
                response = await asyncio.to_thread(
                    self.gemini_model.generate_content,
                    prompt
                )
                logger.info(f"_do_narrative_analysis response.text = {response.text}")
                return response.text.strip()
            except Exception as e:
                logger.error(f"Gemini APIè°ƒç”¨å¤±è´¥: {e}")
                raise

        try:
            logger.info(f'gemini analyze prompt {prompt}')
            # ä½¿ç”¨å¹¶å‘æ§åˆ¶çš„AIè¯·æ±‚
            result_text = await self._controlled_ai_request(_do_narrative_analysis)
            # å°è¯•è§£æJSON
            if result_text.startswith("```json"):
                result_text = result_text[7:-3]
            elif result_text.startswith("```"):
                result_text = result_text[3:-3]

            result_data = json.loads(result_text)

            return NarrativeAnalysis(**result_data)

        except Exception as e:
            logger.error(f"å™äº‹åˆ†æå¤±è´¥: {e}")
            return NarrativeAnalysis(
                category="UNKNOWN",
                description="æ— æ³•åˆ†æå™äº‹èƒŒæ™¯",
                relevance_score=0.0,
                trend_analysis="åˆ†æå¤±è´¥"
            )
            
    async def _assess_risk(self, token_data: TokenData, search_results: List[WebSearchResult]) -> RiskLevel:
        """è¯„ä¼°ä»£å¸é£é™©"""
        search_context = "\n".join([
            f"æ ‡é¢˜: {result.title}\næ‘˜è¦: {result.snippet}"
            for result in search_results[:5]
        ])
        
        prompt = f"""
        è¯„ä¼°ä»¥ä¸‹åŠ å¯†è´§å¸ä»£å¸çš„æŠ•èµ„é£é™©ï¼š
        
        ä»£å¸ä¿¡æ¯:
        - åç§°: {token_data.name}
        - ç¬¦å·: {token_data.symbol}
        - æ€»ä¾›åº”é‡: {token_data.token_total_supply}
        - è™šæ‹Ÿä»£å¸å‚¨å¤‡: {token_data.virtual_token_reserves}
        - è™šæ‹ŸSOLå‚¨å¤‡: {token_data.virtual_sol_reserves}
        
        ç½‘ç»œæœç´¢ç»“æœ:
        {search_context}
        
        è¯·ä»ä»¥ä¸‹è§’åº¦è¯„ä¼°é£é™©ï¼š
        1. é¡¹ç›®åˆæ³•æ€§
        2. æµåŠ¨æ€§é£é™©
        3. æŠ€æœ¯é£é™©
        4. å¸‚åœºé£é™©
        5. ç›‘ç®¡é£é™©
        
        é£é™©ç­‰çº§: LOW, MEDIUM, HIGH, CRITICAL
        é£é™©è¯„åˆ†: 0-100 (100ä¸ºæœ€é«˜é£é™©)
        
        è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
        {{
            "level": "é£é™©ç­‰çº§",
            "score": è¯„åˆ†,
            "description": "è¯¦ç»†é£é™©æè¿°"
        }}
        """
        
        # å®šä¹‰å®é™…çš„AIè¯·æ±‚å‡½æ•°
        async def _do_risk_assessment():
            try:
                response = await asyncio.to_thread(
                    self.gemini_model.generate_content,
                    prompt
                )
                return response.text.strip()
            except Exception as e:
                logger.error(f"Gemini APIè°ƒç”¨å¤±è´¥: {e}")
                raise

        try:
            # ä½¿ç”¨å¹¶å‘æ§åˆ¶çš„AIè¯·æ±‚
            result_text = await self._controlled_ai_request(_do_risk_assessment)
            logger.info(f"assess_risk = {result_text}")
            if result_text.startswith("```json"):
                result_text = result_text[7:-3]
            elif result_text.startswith("```"):
                result_text = result_text[3:-3]

            result_data = json.loads(result_text)

            return RiskLevel(**result_data)

        except Exception as e:
            logger.error(f"é£é™©è¯„ä¼°å¤±è´¥: {e}")
            return RiskLevel(
                level="HIGH",
                score=80.0,
                description="æ— æ³•å®Œæˆé£é™©è¯„ä¼°ï¼Œå»ºè®®è°¨æ…æŠ•èµ„"
            )

    async def _analyze_market(self, token_data: TokenData) -> MarketAnalysis:
        """åˆ†æå¸‚åœºæ•°æ®"""
        # è®¡ç®—ä¸€äº›åŸºæœ¬æŒ‡æ ‡
        if token_data.virtual_sol_reserves > 0 and token_data.virtual_token_reserves > 0:
            estimated_price = token_data.virtual_sol_reserves / token_data.virtual_token_reserves
        else:
            estimated_price = 0

        prompt = f"""
        åˆ†æä»¥ä¸‹ä»£å¸çš„å¸‚åœºæ•°æ®ï¼š

        ä»£å¸ä¿¡æ¯:
        - åç§°: {token_data.name}
        - ç¬¦å·: {token_data.symbol}
        - æ€»ä¾›åº”é‡: {token_data.token_total_supply:,}
        - è™šæ‹Ÿä»£å¸å‚¨å¤‡: {token_data.virtual_token_reserves:,}
        - è™šæ‹ŸSOLå‚¨å¤‡: {token_data.virtual_sol_reserves:,}
        - å®é™…ä»£å¸å‚¨å¤‡: {token_data.real_token_reserves:,}
        - ä¼°ç®—ä»·æ ¼: {estimated_price:.10f} SOL

        è¯·åˆ†æï¼š
        1. æµåŠ¨æ€§çŠ¶å†µ
        2. äº¤æ˜“é‡åˆ†æ
        3. æŒæœ‰è€…åˆ†æ
        4. ä»·æ ¼é¢„æµ‹

        è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
        {{
            "liquidity_analysis": "æµåŠ¨æ€§åˆ†æ",
            "volume_analysis": "äº¤æ˜“é‡åˆ†æ",
            "holder_analysis": "æŒæœ‰è€…åˆ†æ",
            "price_prediction": "ä»·æ ¼é¢„æµ‹"
        }}
        """

        # å®šä¹‰å®é™…çš„AIè¯·æ±‚å‡½æ•°
        async def _do_market_analysis():
            try:
                response = await asyncio.to_thread(
                    self.gemini_model.generate_content,
                    prompt
                )
                return response.text.strip()
            except Exception as e:
                logger.error(f"Gemini APIè°ƒç”¨å¤±è´¥: {e}")
                raise

        try:
            # ä½¿ç”¨å¹¶å‘æ§åˆ¶çš„AIè¯·æ±‚
            result_text = await self._controlled_ai_request(_do_market_analysis)

            if result_text.startswith("```json"):
                result_text = result_text[7:-3]
            elif result_text.startswith("```"):
                result_text = result_text[3:-3]

            result_data = json.loads(result_text)

            return MarketAnalysis(**result_data)

        except Exception as e:
            logger.error(f"å¸‚åœºåˆ†æå¤±è´¥: {e}")
            return MarketAnalysis(
                liquidity_analysis="æµåŠ¨æ€§æ•°æ®ä¸è¶³",
                volume_analysis="äº¤æ˜“é‡æ•°æ®ä¸è¶³",
                holder_analysis="æŒæœ‰è€…æ•°æ®ä¸è¶³",
                price_prediction="æ— æ³•é¢„æµ‹ä»·æ ¼"
            )

    async def _generate_summary(self, token_data: TokenData, narrative: NarrativeAnalysis,
                              risk: RiskLevel, market: MarketAnalysis,
                              search_results: List[WebSearchResult]) -> str:
        """ç”ŸæˆAIæ€»ç»“"""
        prompt = f"""
        åŸºäºä»¥ä¸‹åˆ†æç»“æœï¼Œä¸ºä»£å¸ {token_data.name} ({token_data.symbol}) ç”Ÿæˆä¸€ä¸ªå…¨é¢çš„æ€»ç»“ï¼š

        å™äº‹åˆ†æ:
        - ç±»åˆ«: {narrative.category}
        - æè¿°: {narrative.description}
        - ç›¸å…³æ€§è¯„åˆ†: {narrative.relevance_score}
        - è¶‹åŠ¿åˆ†æ: {narrative.trend_analysis}

        é£é™©è¯„ä¼°:
        - é£é™©ç­‰çº§: {risk.level}
        - é£é™©è¯„åˆ†: {risk.score}
        - é£é™©æè¿°: {risk.description}

        å¸‚åœºåˆ†æ:
        - æµåŠ¨æ€§: {market.liquidity_analysis}
        - äº¤æ˜“é‡: {market.volume_analysis}
        - æŒæœ‰è€…: {market.holder_analysis}
        - ä»·æ ¼é¢„æµ‹: {market.price_prediction}

        è¯·ç”Ÿæˆä¸€ä¸ªç®€æ´ä½†å…¨é¢çš„æ€»ç»“ï¼ŒåŒ…æ‹¬ï¼š
        1. é¡¹ç›®æ¦‚è¿°
        2. ä¸»è¦ä¼˜åŠ¿å’Œé£é™©
        3. å¸‚åœºè¡¨ç°åˆ†æ
        4. å…³é”®æ³¨æ„äº‹é¡¹

        æ€»ç»“åº”è¯¥å®¢è§‚ã€ä¸“ä¸šï¼Œé€‚åˆæŠ•èµ„è€…å‚è€ƒã€‚
        """

        # å®šä¹‰å®é™…çš„AIè¯·æ±‚å‡½æ•°
        async def _do_summary_generation():
            try:
                response = await asyncio.to_thread(
                    self.gemini_model.generate_content,
                    prompt
                )
                return response.text.strip()
            except Exception as e:
                logger.error(f"Gemini APIè°ƒç”¨å¤±è´¥: {e}")
                raise

        try:
            # ä½¿ç”¨å¹¶å‘æ§åˆ¶çš„AIè¯·æ±‚
            result = await self._controlled_ai_request(_do_summary_generation)
            return result
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ€»ç»“å¤±è´¥: {e}")
            return f"æ— æ³•ç”Ÿæˆ {token_data.symbol} çš„è¯¦ç»†åˆ†ææ€»ç»“ã€‚å»ºè®®è¿›è¡Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚"

    async def _generate_investment_recommendation(self, token_data: TokenData, narrative: NarrativeAnalysis,
                                                risk: RiskLevel, market: MarketAnalysis) -> str:
        """ç”ŸæˆæŠ•èµ„å»ºè®®"""
        prompt = f"""
        åŸºäºä»¥ä¸‹åˆ†æï¼Œä¸ºä»£å¸ {token_data.name} ({token_data.symbol}) æä¾›æŠ•èµ„å»ºè®®ï¼š

        é£é™©ç­‰çº§: {risk.level} (è¯„åˆ†: {risk.score})
        å™äº‹ç›¸å…³æ€§: {narrative.relevance_score}
        é¡¹ç›®ç±»åˆ«: {narrative.category}

        è¯·æä¾›ï¼š
        1. æ˜ç¡®çš„æŠ•èµ„å»ºè®® (å¼ºçƒˆæ¨è/æ¨è/ä¸­æ€§/ä¸æ¨è/å¼ºçƒˆä¸æ¨è)
        2. å»ºè®®çš„æŠ•èµ„ç­–ç•¥
        3. é£é™©ç®¡ç†å»ºè®®
        4. å…³é”®ç›‘æ§æŒ‡æ ‡

        å»ºè®®åº”è¯¥åŸºäºé£é™©è¯„ä¼°å’Œå¸‚åœºåˆ†æï¼Œä¿æŒå®¢è§‚å’Œä¸“ä¸šã€‚
        """
        logger.info(f"Generate content for {prompt}")

        # å®šä¹‰å®é™…çš„AIè¯·æ±‚å‡½æ•°
        async def _do_recommendation_generation():
            try:
                response = await asyncio.to_thread(
                    self.gemini_model.generate_content,
                    prompt
                )
                return response.text.strip()
            except Exception as e:
                logger.error(f"Gemini APIè°ƒç”¨å¤±è´¥: {e}")
                raise

        try:
            # ä½¿ç”¨å¹¶å‘æ§åˆ¶çš„AIè¯·æ±‚
            result = await self._controlled_ai_request(_do_recommendation_generation)
            return result
        except Exception as e:
            logger.error(f"ç”ŸæˆæŠ•èµ„å»ºè®®å¤±è´¥: {e}")
            return "ç”±äºåˆ†æé™åˆ¶ï¼Œæ— æ³•æä¾›å…·ä½“æŠ•èµ„å»ºè®®ã€‚è¯·è°¨æ…æŠ•èµ„å¹¶è¿›è¡Œè‡ªå·±çš„ç ”ç©¶ã€‚"



