"""
é«˜æ•ˆGoogleæœç´¢å¼•æ“
åŸºäºgooglesearchåº“çš„ä¼˜åŒ–ç‰ˆæœ¬
"""

from time import sleep
from bs4 import BeautifulSoup
from requests import get
from urllib.parse import unquote
import random
from backend.utils.logger import setup_logger

logger = setup_logger(__name__)


def get_useragent():
    """
    ç”Ÿæˆéšæœºç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²ï¼Œæ¨¡æ‹Ÿå„ç§è½¯ä»¶ç‰ˆæœ¬çš„æ ¼å¼ã€‚
    
    Returns:
        str: éšæœºç”Ÿæˆçš„ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²
    """
    lynx_version = f"Lynx/{random.randint(2, 3)}.{random.randint(8, 9)}.{random.randint(0, 2)}"
    libwww_version = f"libwww-FM/{random.randint(2, 3)}.{random.randint(13, 15)}"
    ssl_mm_version = f"SSL-MM/{random.randint(1, 2)}.{random.randint(3, 5)}"
    openssl_version = f"OpenSSL/{random.randint(1, 3)}.{random.randint(0, 4)}.{random.randint(0, 9)}"
    return f"{lynx_version} {libwww_version} {ssl_mm_version} {openssl_version}"


def _req(term, results, lang, start, proxies, timeout, safe, ssl_verify, region):
    """å‘é€Googleæœç´¢è¯·æ±‚"""
    try:
        resp = get(
            url="https://www.google.com/search",
            headers={
                "User-Agent": get_useragent(),
                "Accept": "*/*"
            },
            params={
                "q": term,
                "num": results + 2,  # é˜²æ­¢å¤šæ¬¡è¯·æ±‚
                "hl": lang,
                "start": start,
                "safe": safe,
                "gl": region,
            },
            proxies=proxies,
            timeout=timeout,
            verify=ssl_verify,
            cookies={
                'CONSENT': 'PENDING+987',  # ç»•è¿‡åŒæ„é¡µé¢
                'SOCS': 'CAESHAgBEhIaAB',
            }
        )
        resp.raise_for_status()
        logger.debug(f"Googleæœç´¢è¯·æ±‚æˆåŠŸ: {term}")
        return resp
    except Exception as e:
        logger.error(f"Googleæœç´¢è¯·æ±‚å¤±è´¥: {e}")
        raise


class SearchResult:
    """æœç´¢ç»“æœç±»"""
    
    def __init__(self, url, title, description):
        self.url = url
        self.title = title
        self.description = description

    def __repr__(self):
        return f"SearchResult(url={self.url}, title={self.title}, description={self.description})"

    def __str__(self):
        return f"{self.title},{self.url},{self.description}\n"


def search(term, num_results=10, lang="en", proxy=None, advanced=False, 
          sleep_interval=0, timeout=5, safe="active", ssl_verify=None, 
          region=None, start_num=0, unique=False):
    """
    æœç´¢Googleæœç´¢å¼•æ“
    
    Args:
        term: æœç´¢è¯
        num_results: ç»“æœæ•°é‡
        lang: è¯­è¨€
        proxy: ä»£ç†
        advanced: æ˜¯å¦è¿”å›SearchResultå¯¹è±¡
        sleep_interval: è¯·æ±‚é—´éš”
        timeout: è¶…æ—¶æ—¶é—´
        safe: å®‰å…¨æœç´¢
        ssl_verify: SSLéªŒè¯
        region: åœ°åŒº
        start_num: èµ·å§‹ä½ç½®
        unique: æ˜¯å¦å»é‡
        
    Yields:
        SearchResultæˆ–str: æœç´¢ç»“æœ
    """
    logger.info(f"ğŸ” å¼€å§‹Googleæœç´¢: {term} (éœ€è¦{num_results}ä¸ªç»“æœ)")
    
    # ä»£ç†è®¾ç½®
    proxies = None
    if proxy and (proxy.startswith("https") or proxy.startswith("http") or proxy.startswith("socks5")):
        proxies = {"https": proxy, "http": proxy}

    start = start_num
    fetched_results = 0  # è·Ÿè¸ªæ€»è·å–ç»“æœæ•°
    fetched_links = set()  # è·Ÿè¸ªå·²è§è¿‡çš„é“¾æ¥

    while fetched_results < num_results:
        try:
            # å‘é€è¯·æ±‚
            resp = _req(term, num_results - start, lang, start, proxies, timeout, safe, ssl_verify, region)
            
            # è§£æHTML
            soup = BeautifulSoup(resp.text, "html.parser")
            result_block = soup.find_all("div", class_="ezO2md")
            new_results = 0  # è·Ÿè¸ªæœ¬æ¬¡è¿­ä»£çš„æ–°ç»“æœæ•°

            logger.debug(f"æ‰¾åˆ° {len(result_block)} ä¸ªæœç´¢ç»“æœå—")

            for result in result_block:
                try:
                    # åœ¨ç»“æœå—ä¸­æŸ¥æ‰¾é“¾æ¥æ ‡ç­¾
                    link_tag = result.find("a", href=True)
                    # åœ¨é“¾æ¥æ ‡ç­¾ä¸­æŸ¥æ‰¾æ ‡é¢˜æ ‡ç­¾
                    title_tag = link_tag.find("span", class_="CVA68e") if link_tag else None
                    # åœ¨ç»“æœå—ä¸­æŸ¥æ‰¾æè¿°æ ‡ç­¾
                    description_tag = result.find("span", class_="FrIlee")

                    # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°æ‰€æœ‰å¿…è¦çš„æ ‡ç­¾
                    if link_tag and title_tag and description_tag:
                        # æå–å¹¶è§£ç é“¾æ¥URL
                        link = unquote(link_tag["href"].split("&")[0].replace("/url?q=", "")) if link_tag else ""
                        
                        # æ£€æŸ¥é“¾æ¥æ˜¯å¦å·²è·å–è¿‡ï¼Œå¦‚æœéœ€è¦å”¯ä¸€ç»“æœ
                        if link in fetched_links and unique:
                            continue  # å¦‚æœé“¾æ¥ä¸å”¯ä¸€åˆ™è·³è¿‡æ­¤ç»“æœ
                        
                        # å°†é“¾æ¥æ·»åŠ åˆ°å·²è·å–é“¾æ¥é›†åˆ
                        fetched_links.add(link)
                        
                        # æå–æ ‡é¢˜æ–‡æœ¬
                        title = title_tag.text if title_tag else ""
                        # æå–æè¿°æ–‡æœ¬
                        description = description_tag.text if description_tag else ""
                        
                        # å¢åŠ è·å–ç»“æœè®¡æ•°
                        fetched_results += 1
                        # å¢åŠ æœ¬æ¬¡è¿­ä»£æ–°ç»“æœè®¡æ•°
                        new_results += 1
                        
                        # æ ¹æ®advancedæ ‡å¿—è¿”å›ç»“æœ
                        if advanced:
                            yield SearchResult(link, title, description)  # è¿”å›SearchResultå¯¹è±¡
                        else:
                            yield link  # åªè¿”å›é“¾æ¥

                        if fetched_results >= num_results:
                            break  # å¦‚æœå·²è·å–æ‰€éœ€æ•°é‡çš„ç»“æœåˆ™åœæ­¢
                            
                except Exception as e:
                    logger.warning(f"è§£ææœç´¢ç»“æœé¡¹å¤±è´¥: {e}")
                    continue

            if new_results == 0:
                logger.warning(f"æŸ¥è¯¢ '{term}' åªæ‰¾åˆ° {fetched_results} ä¸ªç»“æœï¼Œéœ€è¦ {num_results} ä¸ªç»“æœ")
                break  # å¦‚æœæœ¬æ¬¡è¿­ä»£æ²¡æœ‰æ‰¾åˆ°æ–°ç»“æœåˆ™è·³å‡ºå¾ªç¯

            start += 10  # å‡†å¤‡ä¸‹ä¸€ç»„ç»“æœ
            if sleep_interval > 0:
                sleep(sleep_interval)
                
        except Exception as e:
            logger.error(f"æœç´¢è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            break

    logger.info(f"âœ… Googleæœç´¢å®Œæˆ: {term} (è·å¾—{fetched_results}ä¸ªç»“æœ)")


def search_crypto_info(token_symbol,token_ca,num_results=12):
    """
    ä¸“é—¨ç”¨äºåŠ å¯†è´§å¸ä¿¡æ¯æœç´¢çš„ä¾¿æ·å‡½æ•°
    
    Args:
        token_name: ä»£å¸åç§°
        token_symbol: ä»£å¸ç¬¦å·
        num_results: ç»“æœæ•°é‡
        
    Returns:
        List[SearchResult]: æœç´¢ç»“æœåˆ—è¡¨
    """
    queries = [
        f"${token_symbol}[{token_ca}] cryptocurrency",
        f"${token_symbol}[{token_ca}] token pump.fun",
        # f"${token_symbol}[{token_ca}] crypto project analysis",
        # f"${token_symbol}[{token_ca}] meme coin review"
    ]
    
    all_results = []
    
    for query in queries:
        try:
            results = list(search(
                query,
                num_results=max(6, num_results // len(queries)),
                lang='en',
                advanced=True,
                sleep_interval=0.5,
                timeout=10,
                safe='active',
                unique=True
            ))
            all_results.extend(results)
            
            if len(all_results) >= num_results:
                break
                
        except Exception as e:
            logger.error(f"æœç´¢æŸ¥è¯¢ '{query}' å¤±è´¥: {e}")
            continue
    
    return all_results[:num_results]
